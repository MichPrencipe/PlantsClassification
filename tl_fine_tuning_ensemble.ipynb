{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6969317,"sourceType":"datasetVersion","datasetId":4004225},{"sourceId":6987635,"sourceType":"datasetVersion","datasetId":4015956},{"sourceId":6995790,"sourceType":"datasetVersion","datasetId":4021280}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{"id":"88BKu5Ww_lou"}},{"cell_type":"code","source":"# Fix randomness and hide warnings\nseed = 42\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\nimport numpy as np\nnp.random.seed(seed)\n\nimport logging\n\nimport random\nrandom.seed(seed)\nimport datetime","metadata":{"id":"XY8f-aiY_low","executionInfo":{"status":"ok","timestamp":1699855034093,"user_tz":-60,"elapsed":9,"user":{"displayName":"Lorenzo Campana","userId":"02770932588916767915"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==2.14.0 -U -q\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\ntf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import tensorflow\n!pip install keras-cv keras-tuner -q -U\n\nimport keras_cv\nimport keras_tuner as kt","metadata":{"id":"fWwdXy3L_low","outputId":"073d42d1-f56d-452d-9eef-5c46e99be1c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import other libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\nimport seaborn as sns","metadata":{"id":"txZnZLJH_lox","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting","metadata":{"id":"CNgI7YkI_lox"}},{"cell_type":"code","source":"processed_data = np.load('/kaggle/input/processed-data/processed_data.npz')\nX = processed_data['X']\ny = processed_data['y']\nlabels = {0:'healthy', 1:'unhealthy'}\ny.shape","metadata":{"id":"I4otjL6l_lox","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_val, X_test, y_train_val, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.1,\n    stratify=y,\n    random_state=seed\n)\n# Further split the combined training and validation set into a training set and a validation set\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_val,\n    y_train_val,\n    test_size = len(X_test), # Ensure validation set size matches test set size\n    stratify=y_train_val,\n    random_state=seed\n)\n\n# Print the shapes of the resulting datasets\nprint(\"Training_Validation Data Shape:\", X_train_val.shape)\nprint(\"Training_Validation Label Shape:\", y_train_val.shape)\nprint(\"Train Data Shape:\", X_train.shape)\nprint(\"Train Label Shape:\", y_train.shape)\nprint(\"Validation Data Shape:\", X_val.shape)\nprint(\"Validation Label Shape:\", y_val.shape)\nprint(\"Test Data Shape:\", X_test.shape)\nprint(\"Test Label Shape:\", y_test.shape)","metadata":{"id":"dlyanIK2_lox","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the count of occurrences of target classes in the training-validation dataset\nprint('Counting occurrences of y_train classes:')\nprint(pd.DataFrame(np.argmax(y_train, axis=-1), columns=['class']).value_counts())\n\nprint('Counting occurrences of y_val classes:')\nprint(pd.DataFrame(np.argmax(y_val, axis=-1), columns=['class']).value_counts())\n\nprint('Counting occurrences of y_test classes:')\nprint(pd.DataFrame(np.argmax(y_test, axis=-1), columns=['class']).value_counts())\n","metadata":{"id":"3_klV7E-_loz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute alpha for the CategoricalFocalCrossentropy loss function\n# It may be set by inverse class frequency by using compute_class_weight from sklearn.utils. \nclass_weight=compute_class_weight('balanced', classes=np.unique(y_train), y=np.argmax(y_train, axis=-1))\nclass_weight_dict = dict(zip(np.unique(y_train.astype('int')), class_weight))\nalpha = 1 / class_weight\nprint(class_weight, class_weight_dict, alpha)","metadata":{"id":"pZes3a3d59ty","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define batch size, number of epochs, learning rate, input shape, and output shape\nAUTOTUNE = tf.data.AUTOTUNE\nbatch_size = 16\nepochs = 200\nes_patience = 10\nrp_patience = 20\nrp_min_lr = 1e-5\nrp_factor = 0.1\n\n\ninput_shape = X_train.shape[1:]\noutput_shape = 2\n\n# Define two callback functions for early stopping and learning rate reduction\ncallbacks=[\n    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=es_patience, restore_best_weights=True, mode='max'),\n    tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=rp_factor, patience=rp_patience, min_lr=rp_min_lr, mode='max')\n]","metadata":{"id":"cHtNFUTf_loz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation\nPerform Advanced Data Augmentation","metadata":{"id":"7MFVNLgX4d6f"}},{"cell_type":"code","source":"#Add kerascv layers for random augmentation\nrand_augment = keras_cv.layers.RandAugment(\n    value_range=(0, 255),\n    augmentations_per_image=3,\n    magnitude=0.3,\n    magnitude_stddev=0.2,\n    rate=0.5,\n)\nrand_flip = keras_cv.layers.RandomFlip(mode='horizontal')\nmix_up = keras_cv.layers.MixUp()\n\naugmenter = keras_cv.layers.Augmenter(\n    layers=[\n        rand_augment,\n        rand_flip,\n        mix_up,\n    ])","metadata":{"id":"XQbO41wluGwc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augment(images, y, augmenter):\n    inputs = {\"images\": images, \"labels\": y}\n    output = augmenter(inputs)\n    return output[\"images\"], output[\"labels\"]\n\ndef process_validation(images, y):\n    return images, y\n\ndef make_dataset(X, y, mode=\"train\", batch_size=batch_size, augmenter=augmenter):\n    ds = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n        ds = ds.shuffle(batch_size * 4)\n    ds = ds.batch(batch_size)\n    if mode == \"train\":\n        ds = ds.map(lambda x, y: augment(x, y, augmenter), num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        ds = ds.map(process_validation, num_parallel_calls=tf.data.AUTOTUNE)\n        ds = ds.cache()\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef visualize_dataset(dataset, row = 3, col = 3, title=None):\n    for X, _ in dataset.take(1):\n        fig, axs = plt.subplots(row, col, figsize=(5, 5))\n        for i in range(row):\n            for j in range(col):\n              axs[i, j].imshow(X[i * row + j] / 255.0)\n              axs[i, j].axis(\"off\")\n        if title is not None:\n          fig.suptitle(title, fontsize=16)\n        plt.show()\n","metadata":{"id":"F-G7K5OPqH9k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = make_dataset(X_train * 255, y_train)\nvalid_ds = make_dataset(X_val * 255, y_val, mode=\"valid\")","metadata":{"id":"wck0JlroqwDt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset(train_ds, title='All augmentation')","metadata":{"id":"vglRQDvIq6XB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RandAugment","metadata":{"id":"kAKTt6Gx8Dar"}},{"cell_type":"code","source":"rand_aug = keras_cv.layers.Augmenter(layers=[rand_augment])\n\ntrain_ds_after_rand_aug = make_dataset(X_train * 255, y_train, augmenter=rand_aug)\nvisualize_dataset(train_ds_after_rand_aug, title='RandAugment')","metadata":{"id":"WIlQ0KKN6TNc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MixUp","metadata":{"id":"B-y6z6rH4gYx"}},{"cell_type":"code","source":"mix_up_aug = keras_cv.layers.Augmenter(layers=[mix_up])\ntrain_ds_after_mix_up = make_dataset(X_train * 255, y_train, augmenter=mix_up_aug)\nvisualize_dataset(train_ds_after_mix_up, title='MixUp')","metadata":{"id":"qqNZrdg54kPy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RandFlip","metadata":{"id":"hA7-tT5fx8mW"}},{"cell_type":"code","source":"flip_aug = keras_cv.layers.Augmenter(layers=[rand_flip])\n\ntrain_ds_after_flip = make_dataset(X_train * 255, y_train, augmenter=flip_aug)\nvisualize_dataset(train_ds_after_flip, title='RandomHorizontalFlip')","metadata":{"id":"bu8S50M7x-bQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"PohCyxNH_loz"}},{"cell_type":"code","source":"#HyperSpace found thanks to the hyperparametere tuning\nhs = {\n    'gauss_noise': 0.1,\n    'init': 'he_uniform',\n    'units_1': 128,\n    'units_2': 64,\n    'dropout_1': 0.3,\n    'learning_rate': 1e-4,\n    'weight_decay': 5e-4,\n    'n_layers': 2,\n    'bnorm': True,\n    'dropout': True,\n    'alpha': alpha,\n}\n\nbig_hs = {\n    'gauss_noise': 0.1,\n    'init': 'he_uniform',\n    'units_1': 256,\n    'units_2': 128,\n    'dropout_1': 0.2,\n    'learning_rate': 1e-4,\n    'weight_decay': 5e-4,\n    'n_layers': 2,\n    'bnorm': True,\n    'dropout': True,\n    'alpha': alpha,\n}\n\n#Best model for the ensembling according to previous trainings \nbaseline_models = {\n    'convnext_xlarge': {\n        'baseline_model': tfk.applications.ConvNeXtXLarge(\n            weights='imagenet',\n            include_top=False,\n            input_shape=input_shape,\n        ),\n        'to_freeze': 228,\n        'hs': big_hs,\n        'weight': 0.4\n    },\n    'efficientnetv2-l': {\n        'baseline_model': tfk.applications.EfficientNetV2L(\n            weights='imagenet',\n            include_top=False,\n            input_shape=input_shape,\n        ),\n        'to_freeze': 663,\n        'hs': hs,\n        'weight': 0.2\n    },\n    'convnext_base': {\n        'baseline_model': tfk.applications.ConvNeXtBase(\n            weights='imagenet',\n            include_top=False,\n            input_shape=input_shape,\n        ),\n        'to_freeze': 220,\n        'hs': big_hs,\n        'weight': 0.4\n\n    },\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(\n    model_name,\n    baseline_models,\n    input_shape=input_shape,\n    output_shape=output_shape,\n):\n    hs = baseline_models[model_name]['hs']\n    \n    preprocessing = tfk.Sequential([\n        tfkl.GaussianNoise(hs['gauss_noise'], name=f'{model_name}_gauss_noise'),\n    ], name=f'{model_name}_preprocessing')\n\n    input_layer = tfkl.Input(shape=input_shape, name=f'{model_name}_input_layer')\n\n    baseline_model = baseline_models[model_name]['baseline_model']\n\n    baseline_model.trainable = False\n    \n    x = preprocessing(input_layer)\n\n    x = baseline_model(x)\n\n    x = tfkl.GlobalAveragePooling2D(name=f'{model_name}_gap')(x)\n\n    x = tfkl.Dense(hs['units_1'], kernel_initializer=hs['init'], name=f'{model_name}_dense_1')(x)\n    if (hs['bnorm']):\n        x = tfkl.BatchNormalization(name=f'{model_name}_batch_norm')(x)\n    x = tfkl.Activation(activation='relu', name=f'{model_name}_activation_1')(x)\n    \n    for i in range(1, hs['n_layers']):\n        if (hs['dropout']):\n            x = tfkl.Dropout(hs[f'dropout_{i}'], name=f'{model_name}_dropout_{i}')(x)\n\n        x = tfkl.Dense(hs[f'units_{i+1}'], kernel_initializer=hs['init'], name=f'{model_name}_dense_{i+1}')(x)\n        x = tfkl.Activation(activation='relu', name=f'{model_name}_activation_{i+1}')(x)\n    \n    \n\n    \n    output_layer = tfkl.Dense(output_shape, name=f'{model_name}_output_layer', activation='softmax')(x)\n\n    # Create the model\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name=model_name)\n    \n    #Using different loss to address the problem of class invariance\n    loss = tfk.losses.CategoricalFocalCrossentropy(alpha=hs['alpha'])\n    model.compile(loss=loss, optimizer=tfk.optimizers.AdamW(hs['learning_rate'], weight_decay=hs['weight_decay']), metrics=['accuracy'])\n    return model","metadata":{"id":"ujQswVeH2zof","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer learning\nPerform transfer learnign for each models defined in baseline_models","metadata":{}},{"cell_type":"code","source":"for model_name in baseline_models:        \n    tl_model = build_model(model_name=model_name, baseline_models=baseline_models)\n    tl_model.summary()\n    \n    train_ds = make_dataset(X_train * 255, y_train)\n    valid_ds = make_dataset(X_val * 255, y_val, mode=\"valid\")\n    \n    tl_history = tl_model.fit(\n        train_ds,\n        validation_data=valid_ds,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=1,\n        callbacks=callbacks,\n    ).history\n    \n    tl_model.save(f'{model_name}_model_tl')\n    del tl_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine tuning\nFine tune previously pre-trained models","metadata":{}},{"cell_type":"code","source":"for model_name in baseline_models.keys():\n    ft_model = tf.keras.models.load_model(f'{model_name}_model_tl')\n    hs = baseline_models[model_name]['hs']\n    baseline_model = ft_model.get_layer(model_name)\n    baseline_model.trainable = True\n    for i, layer in enumerate(baseline_model.layers[:baseline_models[model_name]['to_freeze']+1]):\n        layer.trainable=False\n    # Create the model\n    ft_model.compile(loss=tfk.losses.CategoricalFocalCrossentropy(alpha=hs['alpha']), optimizer=tfk.optimizers.AdamW(1e-5, weight_decay=5e-5), metrics=['accuracy'])\n    ft_model.summary()\n    # Fit the model\n    train_ds = make_dataset(X_train * 255, y_train)\n    valid_ds = make_dataset(X_val * 255, y_val, mode=\"valid\")\n    ft_history = ft_model.fit(\n        train_ds,\n        validation_data=valid_ds,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=1,\n        callbacks=callbacks,\n    ).history\n    \n    ft_model.save(f'{model_name}_model_ft')\n    del ft_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"ft_models = [tf.keras.models.load_model(f'{model_name}_model_ft') for model_name in baseline_models]\npreds = np.array([model.predict(X_test * 255, verbose=1) for model in ft_models])\nweights = np.array([baseline_models[model_name]['weight'] for model_name in baseline_models])\npreds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_preds = np.array([pred * weights[i] for i, pred in enumerate(preds)])\nw_preds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_pred = np.argmax(np.sum(w_preds, axis=0), axis=-1)\nf_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(y_true, y_pred):\n    accuracy = accuracy_score(y_true, y_pred).round(4)\n    precision = precision_score(y_true, y_pred, average='macro').round(4)\n    recall = recall_score(y_true, y_pred, average='macro').round(4)\n    f1 = f1_score(y_true, y_pred, average='macro').round(4)\n\n    return {\n      'accuracy': accuracy,\n      'precision': precision,\n      'recall' : recall,\n      'f1' : f1\n    }\n\ndef compute_cm(y_true, y_pred):\n  # Compute the confusion matrix\n  cm = confusion_matrix(y_true, y_pred)\n  # Plot the confusion matrix\n  plt.figure(figsize=(10, 8))\n  sns.heatmap(cm.T, xticklabels=list(labels.values()), yticklabels=list(labels.values()), cmap='Blues', annot=True)\n  plt.xlabel('True labels')\n  plt.ylabel('Predicted labels')\n  plt.show()\n\ndef make_inference(y_true, y_pred, display_cm=True):\n    metrics = compute_metrics(y_true, y_pred)\n    # Display the computed metrics\n    print(metrics)\n    if (display_cm):\n        compute_cm(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_inference(np.argmax(y_test, axis=-1), f_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we found out?\nEfficientNet2v-l performs poorly with the current hyperparameter space, however, due to the lack of resources, we couldn't attempt to increase\nits performance by means of KerasTuner Hyperband tuner (see other notebook)","metadata":{}},{"cell_type":"markdown","source":"# K Cross Validation\nTo determine the best epoch for each model in the notebook, that we will fine tune again using the entire dataset","metadata":{}},{"cell_type":"code","source":"metadata = {}\n\nfor model_name in baseline_models.keys():\n    # Define the number of folds for cross-validation\n    num_folds = 5\n\n    # Initialize lists to store training histories, scores, and best epochs\n    histories = []\n    scores = []\n    best_epochs = []\n\n    # Create a KFold cross-validation object\n    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n    # Loop through each fold\n    for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(X_train_val, y_train_val)):\n        print(f\"Starting training model {model_name} on fold num: {fold_idx+1}\")\n\n        # Build a new dropout model for each fold\n        k_model = tf.keras.models.load_model(f'{model_name}_model_tl')\n        k_model.compile(\n            loss=tfk.losses.CategoricalFocalCrossentropy(alpha=hs['alpha']), \n            optimizer=tfk.optimizers.AdamW(1e-5, weight_decay=5e-5),\n            etrics=['accuracy']\n        )\n\n        train_ds = make_dataset(X_train_val[train_idx] * 255, y_train_val[train_idx])\n        valid_ds = make_dataset(X_train_val[valid_idx] * 255, y_train_val[valid_idx], mode=\"valid\")\n        # Train the model on the training data for this fold\n        history = k_model.fit(\n            train_ds,\n            validation_data=valid_ds,\n            batch_size = batch_size,\n            epochs = epochs,\n            callbacks = callbacks,\n            verbose = 1\n          ).history\n        \n        \n        # Evaluate the model on the validation data for this fold\n        score = k_model.evaluate(valid_ds, verbose=0)\n        print(\"score: {}\".format(score))\n        scores.append(score[1])\n\n        # Calculate the best epoch for early stopping\n        best_epoch = len(history['loss']) - es_patience\n        best_epochs.append(best_epoch)\n\n        # Store the training history for this fold\n        histories.append(history)\n        \n    metadata[model_name] = {\n        'histories': histories,\n        'scores': scores,\n        'best_epochs': best_epochs,\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot histories","metadata":{}},{"cell_type":"code","source":"avg_epochs = {}\nfor model_name, info in metadata.items():\n    # Define a list of colors for plotting\n    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n    scores = info['scores']\n    histories = info['histories']\n    best_epochs = info['best_epochs']\n    \n    # Print mean and standard deviation of Accuracy scores\n    print(f\"Accuracy -> (mean: {np.mean(scores).round(4)}, std: {np.std(scores).round(4)})\")\n\n    # Create a figure for Accuracy visualization\n    plt.figure(figsize=(15,6))\n    # Plot val accuracy for each fold\n    plt.title(f'{model_name} Validation Accuracy')\n    for fold_idx in range(num_folds):\n        print(histories[fold_idx]['val_accuracy'][:-es_patience])\n        plt.plot(histories[fold_idx]['val_accuracy'][:-es_patience], color=colors[fold_idx], label=f'Fold N°{fold_idx+1}')\n        plt.legend(loc='upper right')\n        plt.grid(alpha=.3)\n\n    # Show the plot\n    plt.show()\n    \n    # Calculate the average best epoch\n    avg_epoch = int(np.mean(best_epochs))\n    print(f\"{model_name} best average epoch: {avg_epoch}\")\n    avg_epochs[model_name] = avg_epoch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final training\nTraining with the best average","metadata":{}},{"cell_type":"code","source":"#for each model training with the best average epoch\nfor model_name in baseline_models:\n    print(f\"{model_name} training\")\n    final_model = tf.keras.models.load_model(f\"/kaggle/input/models-tl/{model_name}_model_tl\")\n    \n    final_model.compile(\n        loss=tfk.losses.CategoricalFocalCrossentropy(alpha=hs['alpha']), \n        optimizer=tfk.optimizers.AdamW(1e-5, weight_decay=5e-5),\n        etrics=['accuracy']\n    )\n    final_model.summary()\n    \n    ds = make_dataset(X * 255, y)\n\n    history = final_model.fit(\n        ds,\n        epochs = avg_epochs[model_name],\n        callbacks = callbacks,\n        verbose = 1\n      ).history\n    \n    final_model.save(f'{model_name}_final_model')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}